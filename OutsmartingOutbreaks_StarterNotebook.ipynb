{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoOtsnpOmSMO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.spatial import cKDTree\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77s3XlNnm1Ic"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train = pd.read_csv(\"data/Train.csv\")\n",
        "test = pd.read_csv(\"data/Test.csv\")\n",
        "toilets = pd.read_csv(\"data/toilets.csv\") # Information on toilet locations\n",
        "waste_management = pd.read_csv(\"data/waste_management.csv\") # Additional information on waste management locations in the area.\n",
        "water_sources = pd.read_csv(\"data/water_sources.csv\") # Additional information on water sources in the area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show unique categories for object columns in train_df\n",
        "cols = ['Category_Health_Facility_UUID', 'Disease', 'Month', 'Year']\n",
        "\n",
        "for col in cols:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"Number of unique values: {train[col].nunique()}\")\n",
        "    print(f\"Unique values: {train[col].unique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0b_prP7m1LL"
      },
      "outputs": [],
      "source": [
        "# Combine train and test datasets for consistent preprocessing\n",
        "hospital_data = pd.concat([train, test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQM9IQDSm1NW"
      },
      "outputs": [],
      "source": [
        "# Drop unnecessary columns from supplementary datasets\n",
        "for df in [toilets, waste_management, water_sources]:\n",
        "    df.drop(columns=['Year', 'Month'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZC07j0Am1Pu"
      },
      "outputs": [],
      "source": [
        "# Rename columns for clarity\n",
        "def rename_columns(df, prefix):\n",
        "    for col in df.columns:\n",
        "        if col not in ['Month_Year_lat_lon', 'lat_lon']:\n",
        "            df.rename(columns={col: f\"{prefix}_{col}\"}, inplace=True)\n",
        "\n",
        "rename_columns(toilets, \"toilet\")\n",
        "rename_columns(waste_management, \"waste\")\n",
        "rename_columns(water_sources, \"water\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBbt0l3Um1SP",
        "outputId": "a04607d4-0f49-422f-dff5-8ec88d4eb174"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in the 'Total' count of diseases column\n",
        "hospital_data['Total'].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfTHBfvEm1Uu"
      },
      "outputs": [],
      "source": [
        "# Drop rows with missing latitude and longitude in water sources\n",
        "water_sources.dropna(subset=['water_Transformed_Latitude'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7bOibx7m1Xd"
      },
      "outputs": [],
      "source": [
        "# Visualize locations for a specific year and month\n",
        "# Note the months/year should be in the given timeframe [2019, 2023]\n",
        "def plot_locations(year=2022, month=1, month_name='January'):\n",
        "    if year < 2019 or year > 2023:\n",
        "        print(\"Invalid year. Please choose a year between 2019 and 2023.\")\n",
        "        return\n",
        "\n",
        "    if month < 1 or month > 12:\n",
        "        print(\"Invalid month. Please choose a month between 1 and 12.\")\n",
        "        return\n",
        "\n",
        "    if month_name.capitalize() not in ['January', 'February', 'March',\n",
        "                                       'April', 'May', 'June', 'July',\n",
        "                                       'August', 'September', 'October',\n",
        "                                       'November', 'December']:\n",
        "        print(\"Invalid month name. Please choose from 'January' to 'December'.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    subsets = [\n",
        "        (hospital_data.query(f\"Year == {year} and Month == {month}\"), 'Transformed', 'Hospital', 's'),\n",
        "        (water_sources.query(f\"water_Month_Year == '{month}_{year}'\"), 'water_Transformed', 'Water', 'o'),\n",
        "        (waste_management.query(f\"waste_Month_Year == '{month}_{year}'\"), 'waste_Transformed', 'Waste', 'x'),\n",
        "        (toilets.query(f\"toilet_Month_Year == '{month}_{year}'\"), 'toilet_Transformed', 'Toilet', '^'),\n",
        "    ]\n",
        "    for df, prefix, label, marker in subsets:\n",
        "        plt.scatter(df[f'{prefix}_Longitude'], df[f'{prefix}_Latitude'], label=label, alpha=0.6, marker=marker)\n",
        "    plt.title(f'Locations ({month_name.capitalize()} {year})')\n",
        "    plt.xlabel('Longitude')\n",
        "    plt.ylabel('Latitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "3O4er4lYm1bS",
        "outputId": "eeca8427-b8fb-4e70-b96d-e1c6d2e76b11"
      },
      "outputs": [],
      "source": [
        "plot_locations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "e4Ru14OsPzn0",
        "outputId": "841c68f6-b17f-424a-caa7-9f0d98b2b894"
      },
      "outputs": [],
      "source": [
        "plot_locations(year=2023, month=2, month_name='February')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "7nL7bzNrRikw",
        "outputId": "c796d6ea-3dfd-42f7-dfcb-5f0238296544"
      },
      "outputs": [],
      "source": [
        "plot_locations(year=2023, month=12, month_name='December')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJk_JmwXm1eR"
      },
      "outputs": [],
      "source": [
        "def find_nearest(hospital_df, location_df, lat_col, lon_col, id_col):\n",
        "    # Create a cKDTree for efficient nearest neighbour search\n",
        "    tree = cKDTree(location_df[[lat_col, lon_col]].values)\n",
        "    nearest = {}\n",
        "    # Loop through each hospital and find the nearest site in location_df\n",
        "    for _, row in hospital_df.iterrows():\n",
        "        _, idx = tree.query([row['Transformed_Latitude'], row['Transformed_Longitude']])\n",
        "        nearest[row['ID']] = location_df.iloc[idx][id_col]\n",
        "    return nearest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55h7ySyeoTQS"
      },
      "outputs": [],
      "source": [
        "# Ensure unique identifier columns exist in all supplementary datasets\n",
        "for df, prefix in [(toilets, 'toilet'), (waste_management, 'waste'), (water_sources, 'water')]:\n",
        "    df[f\"{prefix}_Month_Year_lat_lon\"] = (\n",
        "        df[f\"{prefix}_Month_Year\"] + '_' +\n",
        "        df[f\"{prefix}_Transformed_Latitude\"].astype(str) + '_' +\n",
        "        df[f\"{prefix}_Transformed_Longitude\"].astype(str)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "1hAqWjt3U7ry",
        "outputId": "613989e4-7a67-430f-d0c0-64cc9e2c04e1"
      },
      "outputs": [],
      "source": [
        "toilets.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVikZCpJneHF"
      },
      "outputs": [],
      "source": [
        "# Merge datasets with nearest locations\n",
        "merged_data = hospital_data.copy()\n",
        "datasets = [\n",
        "    (toilets, 'toilet', 'toilet_Month_Year_lat_lon'),\n",
        "    (waste_management, 'waste', 'waste_Month_Year_lat_lon'),\n",
        "    (water_sources, 'water', 'water_Month_Year_lat_lon'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nkmm7HxnevS"
      },
      "outputs": [],
      "source": [
        "for df, prefix, id_col in datasets:\n",
        "    nearest = find_nearest(merged_data, df, f\"{prefix}_Transformed_Latitude\", f\"{prefix}_Transformed_Longitude\", id_col)\n",
        "    nearest_df = pd.DataFrame(list(nearest.items()), columns=['ID', id_col])\n",
        "    merged_data = merged_data.merge(nearest_df, on=\"ID\").merge(df, on=id_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plot of Total by Disease\n",
        "# Sort categories by median Total for clearer ordering\n",
        "order = merged_data.groupby('Disease')['Total'].median().sort_values().index.tolist()\n",
        "data = [merged_data.loc[merged_data['Disease'] == c, 'Total'].values for c in order]\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.boxplot(data, tick_labels=order, showfliers=False)  # set showfliers=True if you want to display outliers\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Disease')\n",
        "plt.ylabel('Total')\n",
        "plt.title('Distribution of Total by Disease (sorted by median)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "# Set Date as index\n",
        "df = merged_data.set_index(\"Date\")\n",
        "\n",
        "# Full monthly index over your date range\n",
        "all_months = pd.date_range(df.index.min(), df.index.max(), freq=\"MS\")  # month start\n",
        "\n",
        "# Aggregate over locations → regional totals, one column per disease\n",
        "regional = (\n",
        "    df\n",
        "    .groupby(\"Disease\")[\"Total\"]\n",
        "    .resample(\"MS\")            # monthly start frequency\n",
        "    .sum()\n",
        "    .unstack(\"Disease\")       # index = Date, columns = diseases\n",
        "    .reindex(all_months)      # ensure all months in range\n",
        "    .fillna(0)                # months with no cases → 0\n",
        ")\n",
        "\n",
        "regional.index.name = \"Date\"\n",
        "\n",
        "for disease in regional.columns:\n",
        "    ts = regional[disease]\n",
        "\n",
        "    # Skip diseases that are always zero\n",
        "    if ts.sum() == 0:\n",
        "        continue\n",
        "\n",
        "    plot_pacf(ts, lags=24)\n",
        "    plt.title(f\"Regional ACF – {disease}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZB6-U9UnhC_"
      },
      "source": [
        "## Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_time_series_features(\n",
        "    df: pd.DataFrame,\n",
        "    id_cols=(\"Disease\", \"Location\", \"Category_Health_Facility_UUID\"),\n",
        "    target_col=\"Total\",\n",
        "    year_col=\"Year\",\n",
        "    month_col=\"Month\",\n",
        "    day_col=None,              # set to a column name if you have Day, else uses 1\n",
        "    lags=(1, 2, 3, 6, 12),\n",
        "    rolling_windows=(3, 6),\n",
        "    add_month_sin_cos=True,\n",
        "    add_time_index=True,\n",
        "    drop_na_lags=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Create time-series features for disease outbreak prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Input data with at least id_cols, target_col, Year, Month (and optionally Day).\n",
        "    id_cols : tuple\n",
        "        Columns that define a single time series (per disease-location-facility).\n",
        "    target_col : str\n",
        "        Name of the target variable (e.g. 'Total').\n",
        "    year_col, month_col, day_col : str\n",
        "        Date components. If day_col is None, day=1 is used.\n",
        "    lags : iterable of int\n",
        "        Lags (in months) of the target to create.\n",
        "    rolling_windows : iterable of int\n",
        "        Window sizes (in months) for rolling statistics of the target.\n",
        "    add_month_sin_cos : bool\n",
        "        Whether to add cyclical month features.\n",
        "    add_time_index : bool\n",
        "        Whether to add a global time_index (days since min date).\n",
        "    drop_na_lags : bool\n",
        "        Whether to drop rows where any lag / rolling feature is NaN.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        A copy of df with new feature columns added.\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # --- 1. Build Date column ---\n",
        "    if \"Date\" in df.columns:\n",
        "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "    else:\n",
        "        if day_col is not None and day_col in df.columns:\n",
        "            df[\"Date\"] = pd.to_datetime(\n",
        "                dict(\n",
        "                    year=df[year_col].astype(int),\n",
        "                    month=df[month_col].astype(int),\n",
        "                    day=df[day_col].astype(int),\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            df[\"Date\"] = pd.to_datetime(\n",
        "                dict(\n",
        "                    year=df[year_col].astype(int),\n",
        "                    month=df[month_col].astype(int),\n",
        "                    day=1,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    # Ensure consistent sorting\n",
        "    df = df.sort_values(list(id_cols) + [\"Date\"])\n",
        "\n",
        "    # --- 2. Global time_index / diff_date ---\n",
        "    if add_time_index:\n",
        "        df = df.sort_values(\"Date\")\n",
        "        ref_date = df[\"Date\"].min()\n",
        "        df[\"time_index\"] = (df[\"Date\"] - ref_date).dt.days\n",
        "\n",
        "    # --- 3. Month + seasonality features ---\n",
        "    df[\"month\"] = df[\"Date\"].dt.month\n",
        "    df[\"year\"] = df[\"Date\"].dt.year\n",
        "\n",
        "    if add_month_sin_cos:\n",
        "        df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "        df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "\n",
        "    # --- 4. Lag features per series (Disease-Location-Facility) ---\n",
        "    df = df.sort_values(list(id_cols) + [\"Date\"])\n",
        "    grouped = df.groupby(list(id_cols), group_keys=False)\n",
        "\n",
        "    created_cols = []\n",
        "\n",
        "    # Lags of the target\n",
        "    for lag in lags:\n",
        "        col_name = f\"{target_col}_lag{lag}\"\n",
        "        df[col_name] = grouped[target_col].shift(lag)\n",
        "        created_cols.append(col_name)\n",
        "\n",
        "    # Rolling stats of the target (using past data only)\n",
        "    for window in rolling_windows:\n",
        "        mean_col = f\"{target_col}_roll_mean_{window}\"\n",
        "        std_col = f\"{target_col}_roll_std_{window}\"\n",
        "\n",
        "        df[mean_col] = grouped[target_col].shift(1).rolling(window).mean()\n",
        "        df[std_col] = grouped[target_col].shift(1).rolling(window).std()\n",
        "\n",
        "        created_cols.extend([mean_col, std_col])\n",
        "\n",
        "    # --- 5. Optionally drop rows with missing lag/rolling features ---\n",
        "    if drop_na_lags and created_cols:\n",
        "        df = df.dropna(subset=created_cols).reset_index(drop=True)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ythI0pe-p-Qy"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "target_col = \"Total\"\n",
        "\n",
        "# 1) Feature engineering on the FULL merged data (train + 2023 test)\n",
        "fe_df = make_time_series_features(merged_data)\n",
        "\n",
        "# 2) Drop junk merge-key columns (but KEEP 'ID' for submission)\n",
        "drop_obj_cols = [\n",
        "    \"toilet_Month_Year_lat_lon\", \"toilet_Month_Year\",\n",
        "    \"lat_lon_x\", \"Month_Year_lat_lon_x\",\n",
        "    \"waste_Month_Year_lat_lon\", \"waste_Month_Year\",\n",
        "    \"lat_lon_y\", \"Month_Year_lat_lon_y\",\n",
        "    \"water_Month_Year_lat_lon\", \"water_Month_Year\",\n",
        "    \"lat_lon\", \"Month_Year_lat_lon\",\n",
        "]\n",
        "fe_df = fe_df.drop(columns=[c for c in drop_obj_cols if c in fe_df.columns])\n",
        "\n",
        "# 3) Sort by time (important for lags + TimeSeriesSplit)\n",
        "fe_df = fe_df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# 4) Define train (labelled) vs test (2023 competition) masks\n",
        "#    If 'Total' is NaN for 2023, this also protects you:\n",
        "train_mask = (fe_df[\"Year\"] < 2023) & fe_df[target_col].notna()\n",
        "test_mask  = fe_df[\"Year\"] == 2023\n",
        "\n",
        "# 5) Define categorical & numeric feature columns\n",
        "cat_cols = [\"Disease\", \"Location\", \"Category_Health_Facility_UUID\"]\n",
        "cat_cols = [c for c in cat_cols if c in fe_df.columns]\n",
        "\n",
        "numeric_cols = [\n",
        "    c for c in fe_df.select_dtypes(include=\"number\").columns\n",
        "    if c != target_col\n",
        "]\n",
        "\n",
        "feature_cols = numeric_cols + cat_cols\n",
        "\n",
        "# 6) Build train features / target\n",
        "X_train = fe_df.loc[train_mask, feature_cols]\n",
        "y_train = fe_df.loc[train_mask, target_col]\n",
        "\n",
        "# 7) Build 2023 test features (no y_test here)\n",
        "X_test = fe_df.loc[test_mask, feature_cols]\n",
        "ids_test = fe_df.loc[test_mask, \"ID\"]   # needed for submission\n",
        "\n",
        "# 8) Preprocessor + model pipeline\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", numeric_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"model\", model),\n",
        "])\n",
        "\n",
        "# 9) 5-fold time-series CV on training data only\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "cv_scores = cross_val_score(\n",
        "    pipe,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    cv=tscv,\n",
        "    scoring=mae_scorer,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "print(\"CV MAE per fold:\", -cv_scores)\n",
        "print(\"Mean CV MAE:\", -cv_scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1GslRNGqOXE"
      },
      "source": [
        "#### Make predictions on test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib8SvPj2p-nI"
      },
      "outputs": [],
      "source": [
        "# Fit on all training data (< 2023)\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predict for the 2023 competition test set\n",
        "y_pred_2023 = pipe.predict(X_test)\n",
        "\n",
        "# Build submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": ids_test,          # the ID column from fe_df for 2023 rows\n",
        "    \"Total\": y_pred_2023,    # IMPORTANT: column name must be 'Total'\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission_random_forest.csv\", index=False)\n",
        "\n",
        "print(submission.dtypes)\n",
        "print(submission.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
